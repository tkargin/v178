---
title: Efficient Convex Optimization Requires Superlinear Memory
abstract: We show that any memory-constrained, first-order algorithm which minimizes
  $d$-dimensional, $1$-Lipschitz convex functions over the unit ball to $1/\poly(d)$
  accuracy using at most $d^{1.25 - \delta}$ bits of memory must make at least $\Omega(d^{1
  + \delta})$ first-order queries (for any constant $\delta \in [0, 1/4]$). Consequently,
  the performance of such memory-constrained algorithms are a polynomial factor worse
  than the optimal $\tilde{O}(d)$ query bound for this problem obtained by cutting
  plane methods that use $\tilde{O}(d^2)$ memory. This resolves one of the open problems
  in the COLT 2019 open problem publication of Woodworth and Srebro.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: marsden22a
month: 0
tex_title: Efficient Convex Optimization Requires Superlinear Memory
firstpage: 2390
lastpage: 2430
page: 2390-2430
order: 2390
cycles: false
bibtex_author: Marsden, Annie and Sharan, Vatsal and Sidford, Aaron and Valiant, Gregory
author:
- given: Annie
  family: Marsden
- given: Vatsal
  family: Sharan
- given: Aaron
  family: Sidford
- given: Gregory
  family: Valiant
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/marsden22a/marsden22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
